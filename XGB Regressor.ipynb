{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import trunc\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and save the feature/label dataframes and save into a dictionary\n",
    "def loadFeaturesLabels():\n",
    "    \n",
    "    #read all the feature-label csvs\n",
    "    #read in files and sort into dictionary\n",
    "    #each element of dict is a movie that contains a list of featurelabel screenings\n",
    "    files = np.sort(os.listdir(\"disk/Features & Label Csvs/\"))\n",
    "    prevMovieName = files[1][:files[1].find(\"_\")]\n",
    "    screeningDict = dict()\n",
    "    screenings = list()\n",
    "\n",
    "    for file in files:\n",
    "\n",
    "        if file == '.ipynb_checkpoints':\n",
    "            continue \n",
    "\n",
    "        movieName = file[:file.find(\"_\")]\n",
    "        if movieName != prevMovieName:\n",
    "            screeningDict[prevMovieName] = screenings\n",
    "            screenings = list()\n",
    "            prevMovieName = movieName\n",
    "        else:\n",
    "            infoDf = pd.read_csv(\"disk/Features & Label Csvs/\" + file)\n",
    "            infoDf.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "            infoDf.rename(columns={\"0\": \"VOC\"}, inplace=True)\n",
    "            screenings.append(infoDf)\n",
    "            prevMovieName = movieName\n",
    "    \n",
    "    screeningDict[movieName] = screenings\n",
    "        \n",
    "    return screeningDict\n",
    "\n",
    "\n",
    "#singular features \n",
    "def assembleFeatureLabelDf(screeningDict, movieList):\n",
    "    #assemble entire feature label dataframe \n",
    "    infoDf = pd.DataFrame([])\n",
    "    for movie in movieList:\n",
    "        #access movie \n",
    "        screeningList = screeningDict[movie]\n",
    "        for screening in screeningList:\n",
    "            infoDf = pd.concat([infoDf,screening], axis=0, ignore_index=True)\n",
    "            \n",
    "    return infoDf\n",
    "\n",
    "\n",
    "#window size = 5 mins - 10 instances \n",
    "def createWindowedFeatures(screeningDict, movieList):\n",
    "    #create header for the windowed dataframe\n",
    "    #naming convention Red 1_2 \n",
    "    #Red - colour Red\n",
    "    #1 - position within \n",
    "    singleFeatureHeader = list(screeningDict['Buddy'][0].drop([\"Delta\", \"VOC\"], axis=1).columns)\n",
    "    windowedFeatureHeader = list()\n",
    "    for iteration in range(1,11):\n",
    "        header = [label + \"_\" + str(iteration) for label in singleFeatureHeader]\n",
    "        windowedFeatureHeader += header\n",
    "        \n",
    "    windowedFeatureHeader += [\"Delta\"]\n",
    "\n",
    "    #create windowed features \n",
    "    for movie in movieList:\n",
    "\n",
    "        for screeningIndex in range(0, len(screeningDict[movie])):\n",
    "\n",
    "            screening = screeningDict[movie][screeningIndex]\n",
    "            windowedScreening = pd.DataFrame([])\n",
    "            #create windowed features\n",
    "            for index in range(10, screening.shape[0]):\n",
    "\n",
    "                window = screening[index-10:index].drop([\"Delta\", \"VOC\"], axis=1) #extract window\n",
    "                window = window.values.ravel() #flattened window \n",
    "\n",
    "                #create windowed dataframe\n",
    "                windowedScreening = pd.concat([windowedScreening,pd.DataFrame(np.expand_dims(window, axis=1).T)], axis=0) \n",
    "            \n",
    "            #reindex the windowed screening\n",
    "            windowedScreening.index = range(0, windowedScreening.shape[0])\n",
    "            #add the delta field \n",
    "            delta = pd.DataFrame(np.expand_dims(screening[\"Delta\"][10:].values, axis=1), columns=['Delta'])\n",
    "            windowedScreening = pd.concat([windowedScreening, delta], axis=1, ignore_index=True)   \n",
    "\n",
    "            #add header to dataframe\n",
    "            windowedScreening.columns = windowedFeatureHeader\n",
    "            #assign screening to dict\n",
    "            screeningDict[movie][screeningIndex] = windowedScreening\n",
    "\n",
    "    return screeningDict\n",
    "\n",
    "def dropDuplicateRows(screeningDict):\n",
    "    \n",
    "    #create indices to check repeated rows\n",
    "    movieList = list(screeningDict.keys())\n",
    "\n",
    "    for movie in movieList:\n",
    "\n",
    "        for index in range(0, len(screeningDict[movie])):\n",
    "\n",
    "            screening = screeningDict[movie][index]\n",
    "            \n",
    "            #drop duplicates\n",
    "            screening = screening.drop_duplicates()\n",
    "            #reindex rows\n",
    "            screening.index = range(0, screening.shape[0])\n",
    "\n",
    "            #construct the list \n",
    "            screeningDict[movie][index] = screening\n",
    "            \n",
    "    return screeningDict\n",
    "\n",
    "def calculateDelta(screeningDict):\n",
    "    \n",
    "    movieList = list(screeningDict.keys())\n",
    "    \n",
    "    for movie in movieList:\n",
    "        \n",
    "        for screeningIndex in range(0, len(screeningDict[movie])):\n",
    "            \n",
    "            screening = screeningDict[movie][screeningIndex]\n",
    "    \n",
    "            vocLabels = screening['VOC']\n",
    "            deltaList = list()\n",
    "\n",
    "            delta = None\n",
    "\n",
    "            for index in range(1, vocLabels.shape[0]): \n",
    "                delta = vocLabels.loc[index] - vocLabels.loc[index-1]\n",
    "                deltaList.append(delta)\n",
    "\n",
    "            \n",
    "            #add delta field to dataframe and then assign screening to overall dict\n",
    "            screening.drop(0, axis=0, inplace=True) #drop the first row \n",
    "            screening.index = range(0, screening.shape[0]) #reindex so the join is proper\n",
    "            screening = pd.concat([pd.DataFrame(deltaList, columns=['Delta']),screening], axis=1)\n",
    "            screeningDict[movie][screeningIndex] = screening\n",
    "\n",
    "\n",
    "    return screeningDict\n",
    "\n",
    "def scaleDeltas(screeningDict):\n",
    "    \n",
    "    #scale delta between 0 and 1\n",
    "    movieList = list(screeningDict.keys())\n",
    "\n",
    "    for movie in movieList:\n",
    "        for screeningIndex in range(0, len(screeningDict[movie])):\n",
    "            #access screening\n",
    "            screening = screeningDict[movie][screeningIndex]\n",
    "\n",
    "            #drop na rows\n",
    "            screening.dropna(inplace=True)\n",
    "\n",
    "            #scale\n",
    "            sc = MinMaxScaler()\n",
    "            delta = sc.fit_transform(screening['Delta'].values.reshape(-1,1))\n",
    "            screeningDict[movie][screeningIndex]['Delta'] = delta #assign scaled voc \n",
    "            \n",
    "    return screeningDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the screenings\n",
    "screeningDict = loadFeaturesLabels()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all duplicated rows \n",
    "screeningDict = dropDuplicateRows(screeningDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate deltas\n",
    "screeningDict = calculateDelta(screeningDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale deltas\n",
    "screeningDict = scaleDeltas(screeningDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create windowed features\n",
    "screeningDict = createWindowedFeatures(screeningDict, list(screeningDict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "screeningDict['Buddy'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Delta'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2896\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2897\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Delta'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-214-a79834c8823e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m#NORMAL\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m#split into labels and features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrainingLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatureColName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mtrainingFeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainingDf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatureColName\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtestingLabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestingDf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeatureColName\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2993\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnlevels\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2994\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2995\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2996\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2997\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   2897\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2898\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2899\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2900\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2901\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/index.pyx\u001b[0m in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/hashtable_class_helper.pxi\u001b[0m in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Delta'"
     ]
    }
   ],
   "source": [
    "R2_score = list()\n",
    "RMSE_score = list()\n",
    "testingMovies = list()\n",
    "Random_R2_score = list()\n",
    "Random_RMSE_score = list()\n",
    "\n",
    "movieRuntimesPath = 'Numerical Data/movie_runtimes.csv'\n",
    "movieRuntimeDf = pd.read_csv(movieRuntimesPath, usecols = ['movie', 'runtime (mins)', 'effective runtime'])\n",
    "movieList = list(movieRuntimeDf['movie'])\n",
    "\n",
    "for movie in movieList: \n",
    "    \n",
    "    #train test split \n",
    "    trainingMovies = list(movieRuntimeDf['movie'])\n",
    "    testMovie = [movie]\n",
    "    trainingMovies.pop(trainingMovies.index(testMovie[0]))\n",
    "    \n",
    "    #assemble the training and test feature label dataframes\n",
    "    trainingDf = assembleSingularFeatureLabelDf(screeningDict,trainingMovies)\n",
    "    testingDf = assembleSingularFeatureLabelDf(screeningDict,testMovie)\n",
    "    \n",
    "    featureColName = \"Delta\"\n",
    "    \n",
    "    #NORMAL\n",
    "    #split into labels and features \n",
    "    trainingLabels = trainingDf[featureColName].values\n",
    "    trainingFeatures = trainingDf.drop(featureColName, axis=1).values\n",
    "    testingLabels = testingDf[featureColName].values\n",
    "    testingFeatures = testingDf.drop(featureColName, axis=1).values\n",
    "    #normal \n",
    "    regressor = xgb.XGBRegressor(n_estimators=1000, n_jobs=-1)\n",
    "    regressor.fit(trainingFeatures, trainingLabels.ravel())\n",
    "    #predict\n",
    "    predictions = regressor.predict(testingFeatures)\n",
    "    r2_score = metrics.r2_score(testingLabels, predictions)\n",
    "    rmse = np.sqrt(metrics.mean_squared_error(testingLabels,predictions))\n",
    "    #print and save\n",
    "    print(\"R2 score:\", r2_score)\n",
    "    print(\"RMSE: \", rmse)\n",
    "    R2_score.append(r2_score)\n",
    "    RMSE_score.append(rmse)\n",
    "    testingMovies.append(movie)\n",
    "    \n",
    "    #RANDOM\n",
    "    #randomize\n",
    "    \n",
    "#     randomTrainingVOC = copy.deepcopy(trainingDf[featureColName])\n",
    "#     np.random.shuffle(randomTrainingVOC)\n",
    "#     randomTrainingDf = trainingDf.drop(featureColName, axis=1)\n",
    "#     randomTrainingDf = pd.concat([randomTrainingDf,randomTrainingVOC], axis=1)\n",
    "    \n",
    "#     randomTestingVOC = copy.deepcopy(testingDf[featureColName])\n",
    "#     np.random.shuffle(randomTestingVOC)\n",
    "#     randomTestingDf = testingDf.drop(featureColName, axis=1)\n",
    "#     randomTestingDf = pd.concat([randomTestingDf,randomTestingVOC], axis=1)\n",
    "    \n",
    "#     #split into labels and features \n",
    "#     trainingLabels = randomTrainingDf[featureColName].values\n",
    "#     trainingFeatures = randomTrainingDf.drop(featureColName, axis=1).values\n",
    "#     testingLabels = randomTestingDf[featureColName].values\n",
    "#     testingFeatures = randomTestingDf.drop(featureColName, axis=1).values\n",
    "    \n",
    "#     #normal \n",
    "#     regressor = xgb.XGBRegressor(n_estimators=1000, n_jobs=-1)\n",
    "#     regressor.fit(trainingFeatures, trainingLabels.ravel())\n",
    "#     #predict\n",
    "#     predictions = regressor.predict(testingFeatures)\n",
    "#     r2_score = metrics.r2_score(testingLabels, predictions)\n",
    "#     rmse = np.sqrt(metrics.mean_squared_error(testingLabels,predictions))\n",
    "#     #print and save\n",
    "#     print(\"Random R2 score:\", r2_score)\n",
    "#     print(\"Random RMSE: \", rmse)\n",
    "#     Random_R2_score.append(r2_score)\n",
    "#     Random_RMSE_score.append(rmse)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to dataframe\n",
    "pd.DataFrame({'RMSE':RMSE_score, \n",
    "              'R2 Score':R2_score, \n",
    "              'Random RMSE': Random_RMSE_score,\n",
    "              'Random R2 Score':Random_R2_score,\n",
    "              'Test Movie': testingMovies}).to_csv(\"XGBoost Movie and AR Features Randomisation Results.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
