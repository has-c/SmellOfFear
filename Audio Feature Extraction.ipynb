{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio Feature Extraction\n",
    " \n",
    "## Extract various features from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style(\"dark\")\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "import librosa.display\n",
    "import librosa\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import csv data files\n",
    "movieRuntimePath = 'Numerical Data//movie_runtimes.csv'\n",
    "movieRuntimeDf = pd.read_csv(movieRuntimePath, usecols = ['movie', 'runtime (mins)', 'effective runtime'], header = 0)\n",
    "#create a list of movies\n",
    "movieList = list(movieRuntimeDf['movie'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the raw audio pickle objects\n",
    "\n",
    "sr = 22050 #sampling rate \n",
    "\n",
    "rawAudio = dict()\n",
    "for movie in movieList:\n",
    "    try:\n",
    "        basePath = 'Feature Extraction Pickle Objects//Raw Audio File Pickle Objects//'\n",
    "        moviePath = basePath + movie + '.p'\n",
    "        rawAudio[movie] = pickle.load(open(moviePath,\"rb\"))\n",
    "    except FileNotFoundError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extract features \n",
    "audioFeatures = dict()\n",
    "\n",
    "for movie in movieList:\n",
    "    try:\n",
    "        index = movieList.index(movie)\n",
    "\n",
    "        y = rawAudio[movie]\n",
    "        #split original dataset y into smaller datasets that correspond to the 30s intervals\n",
    "        runtime = movieRuntimeDf.loc[index]['runtime (mins)'] \n",
    "        intervals = runtime * 2\n",
    "        x = np.array_split(y,intervals)\n",
    "\n",
    "        for k in x:\n",
    "\n",
    "            featureDict = dict()\n",
    "\n",
    "            #mel power spectrogram\n",
    "            mel = librosa.feature.melspectrogram(y=k,sr=sr)\n",
    "            #convert to log scale (dB) and use peak power as a reference\n",
    "            logMel = librosa.power_to_db(mel, ref=np.max)\n",
    "\n",
    "            #chroma - pitch class information\n",
    "            chroma = librosa.feature.chroma_cqt(y = k, sr=sr)\n",
    "\n",
    "            #estimated tempo information\n",
    "            tempo, beat_frames = librosa.beat.beat_track(y = k,sr=sr)\n",
    "\n",
    "            #mfcc \n",
    "            mfcc = librosa.feature.mfcc(y=k, sr=sr, n_mfcc = 40) #40 is the amount of cepstral vectors \n",
    "\n",
    "            #spectral centroid - relates to brightness of sound\n",
    "            specCentroid = librosa.feature.spectral_centroid(y = k, sr=sr)\n",
    "\n",
    "            #spectral contrast\n",
    "            specContrast = librosa.feature.spectral_contrast(y = k, sr=sr)\n",
    "\n",
    "            #tonnetz - tonal centroid features\n",
    "            tonnetz = librosa.feature.tonnetz(y = k, sr = sr)\n",
    "\n",
    "            featureDict['logMel'] = logMel\n",
    "            featureDict['chroma'] = chroma\n",
    "            featureDict['tempo'] = tempo\n",
    "            featureDict['mfcc'] = mfcc\n",
    "            featureDict['specCentroid'] = specCentroid\n",
    "            featureDict['specContrast'] = specContrast\n",
    "            featureDict['tonnetz'] = tonnetz\n",
    "\n",
    "        audioFeatures[movie] = featureDict\n",
    "    except KeyError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(audioFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save pickle objects\n",
    "pickle.dump(audioFeatures, open('Feature Extraction Pickle Objects//audioFeatures.p', \"wb\" ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
