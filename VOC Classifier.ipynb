{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from math import trunc\n",
    "import pickle\n",
    "import copy\n",
    "import random\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score,precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grouping(visualList):\n",
    "    framesPerInterval = 30\n",
    "    movieVisuals = list()\n",
    "    for index in range(0, int(len(visualList)/framesPerInterval)):\n",
    "        segment = visualList[index*framesPerInterval:index*framesPerInterval+framesPerInterval]\n",
    "        movieVisuals.append(segment)\n",
    "    return movieVisuals\n",
    "\n",
    "def processVisuals(movieVisualData, runtime, isColour):\n",
    "    visualDataIntervals = grouping(movieVisualData)\n",
    "    #the visual data also has the credits accounted for so remove them\n",
    "    visualDataIntervals = visualDataIntervals[:runtime]\n",
    "    #create a dataframe \n",
    "    if isColour: \n",
    "        #create a dominant colour dataframe\n",
    "        framesPerInterval = 30\n",
    "        header = list();\n",
    "        for i in range(1,framesPerInterval+1):\n",
    "            header = header + ['R'+str(i), 'G' + str(i),  'B'+str(i)]\n",
    "    else: #shade object to be parsed\n",
    "        framesPerInterval = 30\n",
    "        header = ['S' + str(x) for x in range(1,framesPerInterval+1)]\n",
    "    \n",
    "    visualDf = pd.DataFrame(columns=header)\n",
    "    #assemble the dataframe\n",
    "    for segment in visualDataIntervals:\n",
    "        index = visualDataIntervals.index(segment)\n",
    "        colourRow = list()\n",
    "        for colour in segment:\n",
    "            if isColour:\n",
    "                colourRow = colourRow + [colour[0], colour[1], colour[2]]\n",
    "            else:\n",
    "                colourRow = colourRow + [colour[0]]\n",
    "        #assign that colour row to the dataframe\n",
    "        visualDf.loc[index] = colourRow\n",
    "            \n",
    "    return visualDf\n",
    "\n",
    "def processAudio(runtime, audio):\n",
    "    audioFeatures = list(audio.keys())\n",
    "\n",
    "    audioDf = pd.DataFrame(columns=[])        \n",
    "    for key in audioFeatures:\n",
    "        audio[key] = audio[key][:runtime]\n",
    "\n",
    "        #assemble df \n",
    "        #create header\n",
    "        if key != 'tempo':\n",
    "            header = [key + str(x) for x in range(1, len(audio[key][0])+1)]\n",
    "        else:\n",
    "            header = ['tempo']\n",
    "\n",
    "        audioFeatureDf = pd.DataFrame(columns=header)\n",
    "        for index in range(0, len(audio[key])):\n",
    "            feature = audio[key][index]\n",
    "            audioFeatureDf.loc[index] = feature\n",
    "\n",
    "        #concatenate featureDf to audioDf\n",
    "        audioDf = pd.concat([audioDf,audioFeatureDf], axis=1)\n",
    "    \n",
    "    return audioDf\n",
    "\n",
    "def processSubtitles(subs, effectiveRuntime):\n",
    "    \n",
    "    header = ['sentiment value']\n",
    "    subSentimentDf = pd.DataFrame(columns=header)\n",
    "    for sentimentIndex in range(0, len(subs)):\n",
    "        sentiment = subs[sentimentIndex]\n",
    "        if len(sentiment) != 0:\n",
    "            if sentiment['sentimentValue'] == np.NaN:\n",
    "                print('YES')\n",
    "            else:         \n",
    "                subSentimentDf.loc[sentimentIndex] = [sentiment['sentimentValue']]\n",
    "        else:\n",
    "            subSentimentDf.loc[sentimentIndex] = [-1] #indicates no dialog occurred during the scene\n",
    "        \n",
    "        #enforce no dialog until the credit scene if there is in fact no dialog\n",
    "        if len(subSentimentDf) != effectiveRuntime:\n",
    "            #no dialog at the end thus need to fill the rest with -1\n",
    "            for index in range(0, effectiveRuntime-len(subSentimentDf)+1):\n",
    "                 subSentimentDf.loc[index] = [-1]\n",
    "    \n",
    "    return subSentimentDf\n",
    "\n",
    "def processASL(asl, effectiveRuntime):\n",
    "    \n",
    "    header = ['average shot length']\n",
    "    aslDf = pd.DataFrame(columns=header)\n",
    "    for index in range(0, effectiveRuntime): \n",
    "        aslValue = asl[index]\n",
    "        aslDf.loc[index] = aslValue\n",
    "    return aslDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vocRounding is used to truncate the VOCs to 3dp allowing for flexible matching between 2013 and 2015 datasets\n",
    "def vocRounding(vocDf):\n",
    "    vocList = list()\n",
    "    for index in range(0, len(vocDf.columns)):\n",
    "        if vocDf.columns[index] == 'Time' or vocDf.columns[index] == 'ocs' or vocDf.columns[index] == 'co' or vocDf.columns[index] == 'CO2':\n",
    "            vocList.append(vocDf.columns[index])    \n",
    "        else:\n",
    "            #string slice to get the molar mass\n",
    "            voc = vocDf.columns[index]\n",
    "            mass = (trunc(float(voc[1:])*1000))/1000 #TRUNCATE TO 3DP\n",
    "            vocList.append(mass)\n",
    "    return vocList\n",
    "\n",
    "#generate normalised screenings\n",
    "#remove invalid screenings (divide by NaN or divide by 0)\n",
    "#scale vocs between 0 and 1\n",
    "def generateNormalisedScreenings(sliceDict, vocData):\n",
    "    screeningList = list()\n",
    "    matchedMovies = list()\n",
    "    for index in range(0,sliceDict['sliceDf'].shape[0]):\n",
    "        start,end = sliceDict['sliceDf'].loc[index]\n",
    "        screening = vocData[start:end+1]\n",
    "        normalisedFrame = copy.deepcopy(screening)\n",
    "        if max(normalisedFrame.values) != 0 and not(np.isnan(max(normalisedFrame.values))):\n",
    "            normalisedFrame = normalisedFrame.values.reshape(-1,1)\n",
    "            scaler = MinMaxScaler()\n",
    "            normalisedFrame = scaler.fit_transform(normalisedFrame)\n",
    "            normalisedFrame = np.transpose(normalisedFrame)\n",
    "            screeningList.append(normalisedFrame)\n",
    "            matchedMovies.append(sliceDict['matchedMovies'][index])\n",
    "    return screeningList, matchedMovies\n",
    "\n",
    "#train test split - one movie is left out for the test set \n",
    "def movieTrainTestSplit(movieList,matchedMovies,screeningList):\n",
    "    \n",
    "    testScreeningList = list()\n",
    "    testMovieList = list()\n",
    "    testMovie = movieList[random.randint(0, len(movieList)-1)] #pick random test movie \n",
    "    while True:\n",
    "        try:\n",
    "            matchedIndex = matchedMovies.index(testMovie)\n",
    "            screening = screeningList.pop(matchedIndex)\n",
    "            testScreeningList.append(screening)\n",
    "            matchedMovie = matchedMovies.pop(matchedIndex)\n",
    "            testMovieList.append(testMovie)\n",
    "        except ValueError:\n",
    "            break\n",
    "    \n",
    "    return testScreeningList,testMovieList,screeningList,matchedMovies\n",
    "\n",
    "\n",
    "#create overall feature label dataframe\n",
    "def inputOutputDf(screeningList,matchedMovies,movieFeatureDict):\n",
    "    \n",
    "    #create input-output df\n",
    "    info = np.array([])\n",
    "    for i in range(0, len(screeningList)): \n",
    "        \n",
    "        \n",
    "        matchedMovie = matchedMovies[i]\n",
    "        screening = screeningList[i]  \n",
    "        \n",
    "        #Calculate deltas\n",
    "        percentageChange = np.array([(screening[0,x+1] - screening[0,x])/screening[0,x] for x in range(8,screening.shape[1]-1)])\n",
    "        percentageChange[np.isinf(percentageChange)] = 0\n",
    "        #get categorical values\n",
    "        # 1 up\n",
    "        # 0 no change\n",
    "        # -1 down\n",
    "        outputLabels = categoricalValues(percentageChange)\n",
    "        \n",
    "        #AR VOC Input\n",
    "        screening = screening[0] #remove the 1st dimension\n",
    "        listOfInstances = [screening[x:x+9] for x in range(0,len(screening))]\n",
    "        inputDf = np.array([])\n",
    "        for instance in listOfInstances:\n",
    "            if len(instance) == 9:\n",
    "                if inputDf.size == 0:\n",
    "                    inputDf = instance\n",
    "                else:\n",
    "                    inputDf = np.vstack((inputDf,instance))\n",
    "        #cut off last row (as cannot predict it)\n",
    "        inputDf = inputDf[:-1]\n",
    "        \n",
    "\n",
    "        \n",
    "        #only join movie if the movie screening and the length of the features is the same\n",
    "        #concatenate the movie features and the vocs \n",
    "        if info.size == 0:\n",
    "            features = np.hstack((inputDf,movieFeatureDict[matchedMovie].values))\n",
    "            info = np.hstack((features,np.expand_dims(outputLabels, axis=1)))\n",
    "        else:\n",
    "            features = np.hstack((inputDf,movieFeatureDict[matchedMovie].values))\n",
    "            info = np.vstack((info,\n",
    "                             np.hstack((features,np.expand_dims(outputLabels, axis=1)))))\n",
    "                \n",
    "    #convert all values inside the dataset to floats\n",
    "    info = info.astype(float)\n",
    "    \n",
    "    return info \n",
    "\n",
    "\n",
    "def categoricalValues(percentageChange):\n",
    "\n",
    "    percentageChangeFlatUpperLimit = 0.02\n",
    "    percentageChangeFlatLowerLimit = -0.02\n",
    "    # calculate percetange change between 2 values to get the output label\n",
    "    #categories - 'no change', 'up', 'down'\n",
    "    #up = 1\n",
    "    #down = -1\n",
    "    #no change = 0\n",
    "\n",
    "    outputLabels = np.zeros(percentageChange.shape[0])\n",
    "    #up category\n",
    "    upMask = np.greater_equal(percentageChange, percentageChangeFlatUpperLimit)\n",
    "    outputLabels[upMask] = 1\n",
    "    #down category\n",
    "    downMask = np.less_equal(percentageChange, percentageChangeFlatLowerLimit)\n",
    "    outputLabels[downMask] = -1\n",
    "    \n",
    "    return outputLabels\n",
    "\n",
    "def ClassificationModel(featuresTrain,labelsTrain, labelsTest,featuresTest):\n",
    "    print('Train Model')\n",
    "    clf = RandomForestClassifier()\n",
    "    clf.fit(featuresTrain, labelsTrain)  \n",
    "    \n",
    "    print('Test Model')\n",
    "    predictedLabels = clf.predict(featuresTest)\n",
    "    \n",
    "    #compute accuracy and precision\n",
    "    precisionScore = precision_score(labelsTest, predictedLabels)\n",
    "    accuracyScore = accuracy_score(labelsTest, predictedLabels)\n",
    "    \n",
    "    return precisionScore,accuracyScore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CREATE VOC DF\n",
    "\n",
    "#import various numeric csvs\n",
    "vocPath = 'Numerical Data/2013VOCData.csv'\n",
    "voc2013DfAll = pd.read_csv(vocPath, header = 0, nrows = 74208, low_memory=False)\n",
    "movieScreeningsPath = 'Numerical Data/screening_times.csv'\n",
    "movingScreeningsDf = pd.read_csv(movieScreeningsPath, usecols = ['scheduled','movie','filled %'])\n",
    "movieRuntimesPath = 'Numerical Data/movie_runtimes.csv'\n",
    "movieRuntimeDf = pd.read_csv(movieRuntimesPath, usecols = ['movie', 'runtime (mins)', 'effective runtime'])\n",
    "#2015 Dataset\n",
    "starWarsPath = 'Numerical Data/Star Wars-The Force Awakens.csv'\n",
    "starWarsScreeningDf = pd.read_csv(starWarsPath)\n",
    "imOffThenPath = 'Numerical Data/I\\'m Off Then.csv'\n",
    "imOffThenScreeningDf = pd.read_csv(imOffThenPath)\n",
    "helpIShrunkTheTeacherPath = 'Numerical Data/Help, I Shrunk My Teacher.csv'\n",
    "helpIShrunkTheTeacherScreeningDf = pd.read_csv(helpIShrunkTheTeacherPath)\n",
    "vocPath = 'Numerical Data/2015VOCData.csv'\n",
    "voc2015DfAll = pd.read_csv(vocPath)\n",
    "#remove first column of 2015 voc df as its not used\n",
    "voc2015DfAll.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "#full list of movies\n",
    "movieList = list(movieRuntimeDf['movie'])\n",
    "\n",
    "\n",
    "#import the slicing indices\n",
    "slicePath = 'Pickle Objects/VocSlices.p'\n",
    "sliceDict = pickle.load(open(slicePath, \"rb\" )) #contains df of co2 slice indices and matched movie list\n",
    "\n",
    "#round the vocs\n",
    "voc2015Col = vocRounding(voc2015DfAll)\n",
    "voc2013Col = vocRounding(voc2013DfAll)\n",
    "voc2013Df = copy.deepcopy(voc2013DfAll)\n",
    "voc2015Df = copy.deepcopy(voc2015DfAll)\n",
    "voc2013Df.columns = voc2013Col\n",
    "voc2015Df.columns = voc2015Col\n",
    "\n",
    "#rearrange dataframe to be able to merge them successfully\n",
    "voc = voc2015Df.columns[1:]\n",
    "voc2015Df = pd.DataFrame(np.transpose(voc2015Df.values)[1:,:], columns =voc2015Df['Time'])\n",
    "voc2015Df['voc'] = voc\n",
    "voc = index=voc2013Df.columns[1:]\n",
    "voc2013Df = pd.DataFrame(np.transpose(voc2013Df.values)[1:,:], columns =voc2013Df['Time'])\n",
    "voc2013Df['voc'] = voc\n",
    "\n",
    "#join the two voc dataframes (join on the 2013 dataframe)\n",
    "vocDf = pd.merge(voc2013Df, voc2015Df, how='inner', on=['voc'])\n",
    "#drop voc column\n",
    "vocColumn = vocDf['voc']\n",
    "vocDf.drop(\"voc\", axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#reorientate the vocDf, note need to convert all vocs to float\n",
    "vocDf = pd.DataFrame(np.transpose(vocDf.values.astype(float)), columns=vocColumn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resultsList = list()\n",
    "startIndex = 0\n",
    "endIndex = 1\n",
    "randomisationIterations = 100\n",
    "resultsHeader = ['RandomState','VOC','Precision', 'Accuracy']\n",
    "\n",
    "for vocIndex in range(startIndex,endIndex):\n",
    "    for i in range(0,randomisationIterations):\n",
    "        print('Iteration: ', str(i))\n",
    "\n",
    "        voc = vocDf.columns[vocIndex]\n",
    "        print(voc)\n",
    "        vocData = vocDf[voc]\n",
    "\n",
    "        print('Process Data')\n",
    "        #generate normalised screenings\n",
    "        screeningList, matchedMovies = generateNormalisedScreenings(sliceDict, vocData)\n",
    "        \n",
    "        #movie-based train test split\n",
    "        #normal screenings\n",
    "        testScreenings,testMovies,trainScreenings,trainMovies = movieTrainTestSplit(movieList,matchedMovies,screeningList)\n",
    "        \n",
    "        #create input-output df\n",
    "        testSet = inputOutputDf(testScreenings)\n",
    "        trainSet = inputOutputDf(trainScreenings)\n",
    "    \n",
    "        #extract labels and features\n",
    "        featuresTrain = trainSet[:, 0:-1]\n",
    "        labelsTrain = trainSet[:,-1]\n",
    "        featuresTest = testSet[:, 0:-1]\n",
    "        labelsTest = testSet[:,-1]\n",
    "\n",
    "        print('Run classifier')\n",
    "        #regression\n",
    "        precisionScore,accuracyScore = ClassificationModel(featuresTrain,labelsTrain, labelsTest,featuresTest)\n",
    "        resultsList.append([False, voc, precisionScore,accuracyScore])\n",
    "        \n",
    "    print('Write results to file')\n",
    "    #create results Df\n",
    "    resultsDf = pd.DataFrame(resultsList,columns=resultsHeader)\n",
    "    #write df to output file\n",
    "    resultsPath = str(voc) + 'AutoClassifer_MinMaxScaler.csv'\n",
    "    resultsDf.to_csv(resultsPath, sep=',', encoding='utf-8')\n",
    "    print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
